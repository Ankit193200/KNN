{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d825b976-a872-4f8e-ab3e-31aeda2e2ef6",
   "metadata": {},
   "source": [
    "# Que1: What is the KNN algorithm?\n",
    "Ans: Ans: K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n",
    "K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.\n",
    "K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n",
    "K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.\n",
    "K-NN is a non-parametric algorithm, which means it does not make any assumption on underlying data.\n",
    "It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.\n",
    "KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08599bb2-0e1e-4e2c-b6c9-07a6c1bddfbc",
   "metadata": {},
   "source": [
    "# Q2. How do you choose the value of K in KNN?\n",
    "Ans: There is no particular way to determine the best value for \"K\", so we need to try some values to find the best out of them. The most preferred value for K is 5.\n",
    "A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.\n",
    "Large values for K are good, but it may find some difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99f693-292d-4ed5-91b3-cb9f478f5c0c",
   "metadata": {},
   "source": [
    "# Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "Ans: The key differences are: KNN regression tries to predict the value of the output variable by using a local average. KNN classification attempts to predict the class to which the output variable belong by computing the local probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9aa4ba-739d-440e-89f6-98a92ff3af1a",
   "metadata": {},
   "source": [
    "# Q4. How do you measure the performance of KNN?\n",
    "Ans: KNN classifier does not have any specialized training phase as it uses all the training samples for classification and simply stores the results in memory.\n",
    "KNN is a non-parametric algorithm because it does not assume anything about the training data. This makes it useful for problems having non-linear data.\n",
    "KNN can be computationally expensive both in terms of time and storage, if the data is very large because KNN has to store the training data to work. This is generally not the case with other supervised learning models.\n",
    "KNN can be very sensitive to the scale of data as it relies on computing the distances. For features with a higher scale, the calculated distances can be very high and might produce poor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e9338a-eb6f-4e4b-a989-1fb52816a221",
   "metadata": {},
   "source": [
    "# Q5. What is the curse of dimensionality in KNN?\n",
    "Ans: The “Curse of Dimensionality” is a tongue in cheek way of stating that there’s a ton of space in high-dimensional data sets. The size of the data space grows exponentially with the number of dimensions. This means that the size of your data set must also grow exponentially in order to keep the same density. If you don’t, then data points start getting farther and farther apart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ee563-571f-4570-9ee0-dcbb221be9f7",
   "metadata": {},
   "source": [
    "# Q6. How do you handle missing values in KNN?\n",
    "Ans: There are several different approaches to imputing missing\n",
    "values:\n",
    "1. Plug in the mean (quantitative) or most common class\n",
    "(categorical) for all missing values in a variable.\n",
    "2. Create a new variable that is an indicator of missingness,\n",
    "and include it in any model to predict the response (also\n",
    "plug in zero or the mean in the actual variable).\n",
    "3. Hot deck imputation: for each missing entry, randomly\n",
    "select an observed entry in the variable and plug it in.\n",
    "4. Model the imputation: plug in predicted values (yˆ) from\n",
    "a model based on the other observed predictors.\n",
    "5. Model the imputation with uncertainty: plug in predicted\n",
    "values plus randomness (yˆ + ε) from a model based on\n",
    "the other observed predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed6f06-33cd-4221-b590-df624fa7115c",
   "metadata": {},
   "source": [
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "Ans: If we go ahead with the traditional KNN approach then this data point will be predicted as red because the majority of the data points are belonging to the red class. But if we think logically, then that does not make sense.\n",
    "\n",
    "Because it is placed very near to the green-colored data point and red-colored data points are very far away. Hence, it should have similar characteristics to a green-colored data point.\n",
    "\n",
    "Moreover, we know that attendance percentage and marks scored in the internal examinations should be proportional to the probability of passing the final examinations. Here, the blue-colored point is placed on the extreme right side of the graph indicates that the student would have already got good marks in the internal exams and a reasonable attendance percentage. It does not make any sense to predict that a particular student will fail the examination just because of the reason that there is a slight edge in the number of red-colored data points.\n",
    "\n",
    "The weighted KNN approach will help us in these kinds of scenarios. This algorithm uses a metric called the “Weighing function”. There are many formulae recommended in the research papers for calculating the value for the Weighing function. In the end, it is a logical metric that is inversely proportional to the distance.\n",
    "\n",
    "Usually, the weight is calculated as the inverse of the distance.\n",
    "\n",
    "For example,\n",
    "\n",
    "If the distance between Point 1 and Point 2 is 20 then wight is calculated as 1/20 ie 0.05.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e507431-d30e-4d76-989a-95d1dc9acb21",
   "metadata": {},
   "source": [
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "Ans: Advantages of KNN\n",
    "\n",
    "1. No Training Period: KNN is called Lazy Learner (Instance based learning). It does not learn anything in the training period. It does not derive any discriminative function from the training data. In other words, there is no training period for it. It stores the training dataset and learns from it only at the time of making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g. SVM, Linear Regression etc.\n",
    "\n",
    "2. Since the KNN algorithm requires no training before making predictions, new data can be added seamlessly which will not impact the accuracy of the algorithm.\n",
    "\n",
    "3. KNN is very easy to implement. There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)\n",
    "\n",
    "Disadvantages of KNN\n",
    "\n",
    "1. Does not work well with large dataset: In large datasets, the cost of calculating the distance between the new point and each existing points is huge which degrades the performance of the algorithm.\n",
    "\n",
    "2. Does not work well with high dimensions: The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate the distance in each dimension.\n",
    "\n",
    "3. Need feature scaling: We need to do feature scaling (standardization and normalization) before applying KNN algorithm to any dataset. If we don't do so, KNN may generate wrong predictions.\n",
    "\n",
    "4. Sensitive to noisy data, missing values and outliers: KNN is sensitive to noise in the dataset. We need to manually impute missing values and remove outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78414f1b-92de-40dc-97e8-4af480d08795",
   "metadata": {},
   "source": [
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "Ans: Manhattan Distance is the L1 norm form (L1 norm is the sum of the magnitude of vectors in space), while Euclidean Distance is L2 Norm form (The L2 norm calculates the distance of the vector coordinate from the origin of the vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c4f79-aa32-4c95-8516-de3194cf42c4",
   "metadata": {},
   "source": [
    "# Q10. What is the role of feature scaling in KNN?\n",
    "Ans: KNN and K-Means are one of the most commonly and widely used machine learning algorithms. KNN is a supervised learning algorithm and can be used to solve both classification as well as regression problems. K-Means, on the other hand, is an unsupervised learning algorithm which is widely used to cluster data into different groups.\n",
    "\n",
    "One thing which is common in both these algorithms is that both KNN and K-Means are distance based algorithms. KNN chooses the k closest neighbors and then based on these neighbors, assigns a class (for classification problems) or predicts a value (for regression problems) for a new observation. K-Means clusters the similar points together. The similarity here is defined by the distance between the points. Lesser the distance between the points, more is the similarity and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c01e9-254a-443a-9dc0-1c3e1a25912d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
